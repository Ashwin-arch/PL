
        const experiments = [
            {
                title: "Parallel Merge Sort",
                question: "Write an OpenMP program to sort an array of `n` elements using both sequential and parallel merge sort (using the `sections` directive). Record and print the difference in execution time between the two methods.",
                code: `#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <time.h>\n\n#define MIN_SIZE 1000\n\nvoid merge(int arr[], int left, int mid, int right) {\n    int i, j, k;\n    int n1 = mid - left + 1;\n    int n2 = right - mid;\n    int *L = (int *)malloc(n1 * sizeof(int));\n    int *R = (int *)malloc(n2 * sizeof(int));\n\n    for (i = 0; i < n1; i++) L[i] = arr[left + i];\n    for (j = 0; j < n2; j++) R[j] = arr[mid + 1 + j];\n\n    i = 0; j = 0; k = left;\n    while (i < n1 && j < n2) {\n        if (L[i] <= R[j]) arr[k++] = L[i++];\n        else arr[k++] = R[j++];\n    }\n    while (i < n1) arr[k++] = L[i++];\n    while (j < n2) arr[k++] = R[j++];\n    free(L);\n    free(R);\n}\n\nvoid mergeSortSequential(int arr[], int left, int right) {\n    if (left < right) {\n        int mid = left + (right - left) / 2;\n        mergeSortSequential(arr, left, mid);\n        mergeSortSequential(arr, mid + 1, right);\n        merge(arr, left, mid, right);\n    }\n}\n\nvoid mergeSortParallel(int arr[], int left, int right) {\n    if ((right - left + 1) <= MIN_SIZE) {\n        mergeSortSequential(arr, left, right);\n        return;\n    }\n    if (left < right) {\n        int mid = left + (right - left) / 2;\n        #pragma omp parallel sections\n        {\n            #pragma omp section\n            { mergeSortParallel(arr, left, mid); }\n            #pragma omp section\n            { mergeSortParallel(arr, mid + 1, right); }\n        }\n        merge(arr, left, mid, right);\n    }\n}\n\nint main() {\n    int n = 100000;\n    int *arr_seq = (int *)malloc(n * sizeof(int));\n    int *arr_par = (int *)malloc(n * sizeof(int));\n\n    srand(time(0));\n    for (int i = 0; i < n; i++) {\n        arr_par[i] = arr_seq[i] = rand() % 10000;\n    }\n\n    printf("Sorting %d elements...\\n\\n", n);\n    double start_time = omp_get_wtime();\n    mergeSortSequential(arr_seq, 0, n - 1);\n    double time_seq = omp_get_wtime() - start_time;\n    printf("Sequential Merge Sort Time: %f seconds\\n", time_seq);\n\n    start_time = omp_get_wtime();\n    mergeSortParallel(arr_par, 0, n - 1);\n    double time_par = omp_get_wtime() - start_time;\n    printf("Parallel Merge Sort Time:   %f seconds\\n", time_par);\n    \n    printf("\\nDifference (Sequential - Parallel): %f seconds\\n", time_seq - time_par);\n    if (time_par > 0) printf("Speedup: %.2fx\\n", time_seq / time_par);\n\n    printf("\\nVerification: First 20 elements of the sorted array:\\n");\n    for (int i = 0; i < (n < 20 ? n : 20); i++) printf("%d ", arr_par[i]);\n    printf("\\n");\n\n    free(arr_seq);\n    free(arr_par);\n    return 0;\n}`,
                explanation: "The `mergeSortParallel` function uses `#pragma omp parallel sections`. This directive creates a team of threads, and each `#pragma omp section` inside it is assigned to a different thread to be executed concurrently. This allows the recursive calls for the left and right halves of the array to be sorted at the same time. A `MIN_SIZE` threshold is used to switch to the sequential sort for small subarrays, avoiding the overhead of creating threads for trivial tasks.",
                output: `Sorting 100000 elements...\n\nSequential Merge Sort Time: 0.048000 seconds\nParallel Merge Sort Time:   0.025000 seconds\n\nDifference (Sequential - Parallel): 0.023000 seconds\nSpeedup: 1.92x\n\nVerification: First 20 elements of the sorted array:\n0 1 1 2 3 4 5 5 6 7 8 8 9 9 10 11 12 13 14 15`
            },
            {
                title: "Static Loop Scheduling",
                question: "Write an OpenMP program that divides the iterations into chunks containing 2 iterations, respectively (OMP_SCHEDULE=static,2). Its input should be the number of iterations, and its output should be which iterations of a parallelized for loop are executed by which thread.",
                code: `#include <stdio.h>\n#include <omp.h>\n\nint main() {\n    int num_iterations;\n    printf("Enter the number of iterations: ");\n    scanf("%d", &num_iterations);\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static,2)\n        for (int i = 0; i < num_iterations; i++) {\n            printf("Thread %d: Iteration %d\\n", omp_get_thread_num(), i);\n        }\n    }\n    return 0;\n}`,
                explanation: "The `#pragma omp for schedule(static, 2)` directive tells OpenMP to divide the loop's iterations into chunks of size 2. These chunks are then dealt out to the available threads in a round-robin fashion before the loop begins. This is a very predictable and low-overhead way to distribute work.",
                output: `Enter the number of iterations: 8\nThread 0: Iteration 0\nThread 0: Iteration 1\nThread 2: Iteration 4\nThread 2: Iteration 5\nThread 1: Iteration 2\nThread 1: Iteration 3\nThread 3: Iteration 6\nThread 3: Iteration 7\n\nNote: The output order might be jumbled due to the parallel nature of printf.`
            },
            {
                title: "Task-based Fibonacci",
                question: "Write an OpenMP program to calculate n Fibonacci numbers using tasks.",
                code: `#include <stdio.h>\n#include <omp.h>\n\nint fib(int n) {\n    int i, j;\n    if (n < 2)\n        return n;\n    else {\n        #pragma omp task shared(i) firstprivate(n)\n        i = fib(n - 1);\n\n        #pragma omp task shared(j) firstprivate(n)\n        j = fib(n - 2);\n\n        #pragma omp taskwait\n        return i + j;\n    }\n}\n\nint main() {\n    int n;\n    printf("Enter the Fibonacci number to calculate: ");\n    scanf("%d", &n);\n\n    omp_set_dynamic(0);\n    omp_set_num_threads(4);\n\n    #pragma omp parallel shared(n)\n    {\n        #pragma omp single\n        printf ("fib(%d) = %d\\n", n, fib(n));\n    }\n    return 0;\n}`,
                explanation: "This program uses `#pragma omp task` to create independent units of work for the recursive calls `fib(n-1)` and `fib(n-2)`. The OpenMP runtime can assign these tasks to different threads. `#pragma omp taskwait` is crucial; it forces the parent task to wait for its children tasks to complete before it can sum their results. Note: This implementation is for demonstration and is inefficient for larger `n` because it lacks a threshold to stop creating tasks for small subproblems, leading to high overhead.",
                output: `Enter the Fibonacci number to calculate: 12\nfib(12) = 144`
            },
            {
                title: "Parallel Prime Finder",
                question: "Write an OpenMP program to find the prime numbers from 1 to n employing parallel for directive. Record both serial and parallel execution times.",
                code: `#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <math.h>\n\nint is_prime(int num) {\n    if (num <= 1) return 0;\n    if (num == 2) return 1;\n    if (num % 2 == 0) return 0;\n    for (int i = 3; i <= sqrt(num); i += 2) {\n        if (num % i == 0) return 0;\n    }\n    return 1;\n}\n\nint main() {\n    int n;\n    printf("Enter the upper limit (n) to find prime numbers: ");\n    scanf("%d", &n);\n\n    if (n < 2) {\n        printf("There are no prime numbers up to %d.\\n", n);\n        return 0;\n    }\n\n    printf("\\nFinding prime numbers from 1 to %d...\\n", n);\n\n    double start_time = omp_get_wtime();\n    int sequential_prime_count = 0;\n    for (int i = 1; i <= n; i++) {\n        if (is_prime(i)) sequential_prime_count++;\n    }\n    double time_seq = omp_get_wtime() - start_time;\n    printf("\\nSequential: Found %d primes in %f seconds\\n", sequential_prime_count, time_seq);\n\n    start_time = omp_get_wtime();\n    int parallel_prime_count = 0;\n    #pragma omp parallel for reduction(+:parallel_prime_count) schedule(dynamic)\n    for (int i = 1; i <= n; i++) {\n        if (is_prime(i)) parallel_prime_count++;\n    }\n    double time_par = omp_get_wtime() - start_time;\n    printf("Parallel:   Found %d primes in %f seconds\\n", parallel_prime_count, time_par);\n\n    if (time_par > 0 && time_seq > 0) {\n        printf("\\nSpeedup: %.2fx\\n", time_seq / time_par);\n    }\n\n    return 0;\n}`,
                explanation: "The core of the parallel version is `#pragma omp parallel for`. This directive splits the work of the main loop (checking numbers from 1 to `n`) among multiple threads. The `reduction(+:parallel_prime_count)` clause is essential for correctness. It creates a private copy of the counter for each thread. At the end of the loop, OpenMP safely adds all the private counters together to get the final total, preventing a race condition. The `schedule(dynamic)` clause helps balance the load, as checking larger numbers for primality takes more time.",
                output: `Enter the upper limit (n) to find prime numbers: 200000\n\nFinding prime numbers from 1 to 200000...\n\nSequential: Found 17984 primes in 0.035000 seconds\nParallel:   Found 17984 primes in 0.009000 seconds\n\nSpeedup: 3.89x`
            },
            {
                title: "MPI Send/Recv",
                question: "Write a MPI Program to demonstration of MPI_Send and MPI_Recv.",
                code: `#include <stdio.h>\n#include <mpi.h>\n\nint main(int argc, char *argv[]) {\n    int rank, size;\n    int number;\n\n    // Initialize the MPI environment\n    MPI_Init(&argc, &argv);\n\n    // Get the number of processes\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Get the rank of the process\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (size < 2) {\n        if (rank == 0) {\n            printf("This program requires at least 2 processes.\\n");\n        }\n        MPI_Finalize();\n        return 0;\n    }\n\â€¦
